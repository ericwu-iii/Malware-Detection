import os
import re
import json
import pickle
import requests
import urllib.request
from vt_report_downloader import VTReportDownloader
from tor_init import TorInit
import sys
from datetime import datetime, timedelta
from pathlib import Path


DIR_PATH = os.path.dirname(os.path.realpath(__file__))


class VirusTotalApi:

  def __init__(self, api_key):
    self.api_key = api_key
    self.base_url = 'https://www.virustotal.com/api/v3/'
    self.headers = {'x-apikey': api_key,
                    'Accept': 'application/json'}

  def file_search(self, params):
    # params = dict(apikey=self.api_key, query=query, offset=offset)
    try:
      response = requests.get(self.base_url + 'intelligence/search', headers=self.headers, params=params, timeout=10)
    except requests.RequestException as e:
      return dict(error=str(e))

    return _return_response_and_status_code(response)

  def get_file(self, this_hash):
    """ Download a file by its hash.
    Downloads a file from VirusTotal's store given one of its hashes. This call can be used in conjuction with
    the file searching call in order to download samples that match a given set of criteria.
    :param this_hash: The md5/sha1/sha256 hash of the file you want to download.
    :param timeout: The amount of time in seconds the request should wait before timing out.
    :return: Downloaded file in response.content
    """
    # params = {'hash': this_hash}
    # print(this_hash)
    try:
      response = requests.get(self.base_url + 'files/{}/download_url'.format(this_hash), headers=self.headers, timeout=10)
    except requests.RequestException as e:
      return dict(error=str(e))

    return _return_response_and_status_code(response, json_results=False)

  def file_download(self, res, file_hash):
    '''

    Download and save file

    '''
    now_date = datetime.now().strftime('%Y%m%d')
    file_url = re.search(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", str(res['results'])).group()
    download_info = urllib.request.urlopen(file_url).read()

    if download_info:
      file_writer = open("./{}/{}.danger".format(now_date, file_hash), 'wb')
      file_writer.write(download_info)
      file_writer.close()


def _return_response_and_status_code(response, json_results=True):
  """ Output the requests response content or content as json and status code
  :rtype : dict
  :param response: requests response object
  :param json_results: Should return JSON or raw content
  :return: dict containing the response content and/or the status code with error string.
  """
  if response.status_code == requests.codes.ok:
    return dict(results=response.json() if json_results else response.content, response_code=response.status_code)
  elif response.status_code == 400:
    return dict(
        error='package sent is either malformed or not within the past 24 hours.',
        response_code=response.status_code)
  elif response.status_code == 204:
    return dict(
        error='You exceeded the public API request rate limit (4 requests of any nature per minute)',
        response_code=response.status_code)
  elif response.status_code == 403:
    return dict(
        error='You tried to perform calls to functions for which you require a Private API key.',
        response_code=response.status_code)
  elif response.status_code == 404:
    return dict(error='File not found.', response_code=response.status_code)
  else:
    return dict(response_code=response.status_code)


def main():
  api_key = "a95c1af8bfde3bb5b52f288d79bb09f3a62c971da82b0c3a71d752200b2fd5d0"
  vt_api = VirusTotalApi(api_key)

  crawled_hash_set = pickle.load(open('./crawled_hash_set.pkl', 'rb'))
  unique_text_md5_set = pickle.load(open('./unique_text_md5_set.pkl', 'rb'))

  # start_date = "2013-04-01"
  # year_list = ["2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020"]
  # start_date = ["04-01", "07-01", "10-01", "12-31"]
  # end_date = ["01-01", "04-01", "07-01", "10-01"]

  year_list = ["2020", ]  # "2013", "2015", "2014"
  start_date = ["07-01"]
  end_date = ["03-01"]

  # end_date = datetime.now().strftime('%Y-%m-%d')
  # Documents: text, pdf, ps, doc, docx, rtf, ppt, pptx, xls, xlsx, odp, ods, odt, hwp, gul, ebook, latex.

  # query = """(kaspersky:Ransom AND microsoft:Ransom AND trendmicro:Ransom) AND
  #         (type:doc OR type:docx OR type:pdf OR type:text OR type:ppt OR type:pptx OR type:xls OR type:xlsx)
  #         positives:30+ fs:{}T00:00:00+ ls:{}T23:59:59-""".format(start_date, end_date)
  # query = """
  #         (kaspersky:Ransom AND microsoft:Ransom AND trendmicro:Ransom) AND
  #         positives:30+ fs:{}23:59:59-
  #         """.format(start_date)
  # query = """kaspersky:Ransom AND microsoft:Ransom AND trendmicro:Ransom AND tag:js-embedded"""
  # fs:{}T00:00:00+ AND
  # print(query)

  # params = {
  #   'query':query,
  #   'limit': 200,
  #   'descriptors_only': True,
  #   # 'order': 'first_submission_date-'
  # }

  tor = TorInit(socks_port=9050, control_port=9051)
  tor._startTorProxy()

  for year in year_list:
    for date, end_d in zip(start_date, end_date):

      COUNT_LIMIT = 3000

      query_start_date = '-'.join([year, date])
      query_end_date = '-'.join([year, end_d])
      query = """
            (fireeye:Ransom OR f-secure:Ransom OR avast:Ransom OR alibaba:Ransom OR drweb:Ransom OR symantec:Ransom OR ad-aware:Ransom OR kaspersky:Ransom OR microsoft:Ransom OR trendmicro:Ransom OR bitdefender:Ransom OR mcafee:Ransom OR fortinet:Ransom OR malwarebytes:Ransom) AND
            (type:peexe OR type:pedll OR type:neexe OR type:nedll) AND
            positives:30+ fs:{}T00:00:00- fs:{}T00:00:00+
            """.format(query_start_date, query_end_date)
      # query = """
      #      (kaspersky:clean AND microsoft:clean AND trendmicro:clean) AND
      #      (type:peexe OR type:pedll OR type:neexe OR type:nedll) AND
      #      (positives:5- fs:{}T00:00:00- fs:{}T00:00:00+) AND
      #      size:30MB-
      #      """.format(query_start_date, query_end_date)
      # query = """
      #       (type:peexe OR type:pedll OR type:neexe OR type:nedll) AND
      #       (positives:60+ fs:{}T00:00:00- fs:{}T00:00:00+) AND
      #       size:50MB-
      #       """.format(query_start_date, query_end_date)
      print(query)

      params = {
          'query': query,
          'limit': 150,
          'descriptors_only': False,
          'order': 'first_submission_date-'
      }

      count = 0
      dup_count = 0
      limit_try = 400
      next_url = None

      while limit_try > 0:

        if count >= COUNT_LIMIT:
          break

        try:
          print(next_url)
          file_search_results = vt_api.file_search(params)

          print(file_search_results['results']['meta'])
          try:
            params['cursor'] = file_search_results['results']['meta']['cursor']
          except:
            params['cursor'] = None

          for file in file_search_results['results']['data']:

            # get file hash and download the file
            file_hash = file['id']

            if file_hash in crawled_hash_set:
              print('the {} is crawled'.format(file_hash))
              continue

            else:
              crawled_hash_set.add(file_hash)
              '''

              check duplicate .text code

              '''
              if_try = 1

              while if_try:
                now_date = datetime.now().strftime('%Y%m%d')

                OUTPUT_FOLDER = os.path.join(DIR_PATH, now_date, '')
                Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)

                TXT_PATH = DIR_PATH + '/txt_output_{}/'.format(now_date)
                Path(TXT_PATH).mkdir(parents=True, exist_ok=True)

                _url = 'https://www.virustotal.com/ui/files/{}'.format(file_hash)
                test = VTReportDownloader()
                report = test.get_page_source(_url)

                if report is not None:
                  with open('{}.txt'.format(TXT_PATH + file_hash), 'w') as f:
                    f.write(report)

                if report is None:
                  tor._checkTorStatus()
                  tor._startTorProxy()

                  limit_try -= 1
                  continue

                report = json.loads(report)

                attr = report['data']['attributes']
                if 'pe_info' not in attr.keys():
                  print('file {} has no peid.'.format(file_hash))
                  if_try = 0
                  continue
                if 'sections' not in attr['pe_info'].keys():
                  print('file {} has no sections.'.format(file_hash))
                  if_try = 0
                  continue
                sec = attr['pe_info']['sections']
                if sec[0]['name'] == '.text':
                  md5 = sec[0]['md5']
                  if md5 in unique_text_md5_set:
                    dup_count += 1
                    print('the text md5 {} is duplicated.'.format(md5), dup_count)
                    if dup_count % 50 == 0:
                      pickle.dump(crawled_hash_set, open('crawled_hash_set.pkl', 'wb'))
                      pickle.dump(unique_text_md5_set, open('unique_text_md5_set.pkl', 'wb'))
                      print('set pickles saved.')
                    break

                  unique_text_md5_set.add(md5)
                  print('new text md5 added.', 'total number: {}'.format(len(unique_text_md5_set)))

                  #
                  res = vt_api.get_file(file_hash)
                  # print(res)
                  # break
                  # parse url and save file to .danger
                  vt_api.file_download(res, file_hash)
                  count += 1
                  print('dump num {} : {}.danger'.format(count, file_hash), '/ total danger {}'.format(len(crawled_hash_set)))

                break

          if params['cursor'] is None:
            break

        except Exception as e:
          print('fail dumps, error on line {}: {}, limit try : {}'.format(sys.exc_info()[-1].tb_lineno, e, limit_try))
          if not limit_try:
            print('finish virsu total api dump, total dump num {}'.format(count))
            break
          limit_try -= 1

  pickle.dump(crawled_hash_set, open('crawled_hash_set.pkl', 'wb'))
  pickle.dump(unique_text_md5_set, open('unique_text_md5_set.pkl', 'wb'))
  print('set pickles saved.')


if __name__ == '__main__':
  main()
