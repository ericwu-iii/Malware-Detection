{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:02.751526Z",
     "start_time": "2018-08-15T10:32:01.486456Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:02.761516Z",
     "start_time": "2018-08-15T10:32:02.753895Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    stemmed_words = [stemmer.stemWord(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:02.916025Z",
     "start_time": "2018-08-15T10:32:02.764151Z"
    }
   },
   "outputs": [],
   "source": [
    "# def readIMDB(path, seg='train'):\n",
    "#     pos_or_neg = ['pos', 'neg']\n",
    "#     data = []\n",
    "#     for label in pos_or_neg:\n",
    "#         files = os.listdir(os.path.join(path, seg, label))\n",
    "#         for file in files:\n",
    "#             with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "#                 review = rf.read().replace('\\n', '')\n",
    "#                 if label == 'pos':\n",
    "#                     data.append([clean_text(review), 1])\n",
    "#                 elif label == 'neg':\n",
    "#                     data.append([clean_text(review), 0])\n",
    "#     return data\n",
    "\n",
    "def readIMDB(path, seg='train'):\n",
    "    pos_or_neg = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in pos_or_neg:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([review, 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([review, 0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:04.592988Z",
     "start_time": "2018-08-15T10:32:02.918880Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = readIMDB('aclImdb')\n",
    "test_data = readIMDB('aclImdb', 'test')\n",
    "\n",
    "# random.shuffle(train_data)\n",
    "# random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:07.063230Z",
     "start_time": "2018-08-15T10:32:04.595080Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "train_tokenized = []\n",
    "test_tokenized = []\n",
    "for review, score in train_data:\n",
    "    train_tokenized.append(tokenizer(review))\n",
    "for review, score in test_data:\n",
    "    test_tokenized.append(tokenizer(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:07.557469Z",
     "start_time": "2018-08-15T10:32:07.065325Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = set(chain(*train_tokenized))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:43.454593Z",
     "start_time": "2018-08-15T10:32:07.559379Z"
    }
   },
   "outputs": [],
   "source": [
    "wvmodel = gensim.models.KeyedVectors.load_word2vec_format('test_word.txt',\n",
    "                                                          binary=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:43.654834Z",
     "start_time": "2018-08-15T10:32:43.456902Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:43.660044Z",
     "start_time": "2018-08-15T10:32:43.656887Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:43.795758Z",
     "start_time": "2018-08-15T10:32:43.662049Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:50.625180Z",
     "start_time": "2018-08-15T10:32:43.797848Z"
    }
   },
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))\n",
    "train_labels = torch.tensor([score for _, score in train_data])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))\n",
    "test_labels = torch.tensor([score for _, score in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:50.633753Z",
     "start_time": "2018-08-15T10:32:50.627490Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, weight, labels, use_gpu, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.use_gpu = use_gpu\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:50.771276Z",
     "start_time": "2018-08-15T10:32:50.635500Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:50.929227Z",
     "start_time": "2018-08-15T10:32:50.773710Z"
    }
   },
   "outputs": [],
   "source": [
    "# weight = nn.Embedding(vocab_size+1, embed_size).weight.data\n",
    "weight = torch.zeros(vocab_size+1, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:51.891127Z",
     "start_time": "2018-08-15T10:32:50.932386Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(wvmodel.index2word)):\n",
    "    try:\n",
    "        index = word_to_idx[wvmodel.index2word[i]]\n",
    "    except:\n",
    "        continue\n",
    "    weight[index, :] = torch.from_numpy(wvmodel.get_vector(\n",
    "        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:55.100819Z",
     "start_time": "2018-08-15T10:32:51.893198Z"
    }
   },
   "outputs": [],
   "source": [
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, weight=weight,\n",
    "                   labels=labels, use_gpu=use_gpu)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:55.106026Z",
     "start_time": "2018-08-15T10:32:55.103049Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:32:55.248408Z",
     "start_time": "2018-08-15T10:32:55.107799Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T10:37:09.822630Z",
     "start_time": "2018-08-15T10:32:55.251376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.6863, train acc: 0.55, test loss: 0.6629, test acc: 0.61, time: 50.94\n",
      "epoch: 1, train loss: 0.6124, train acc: 0.67, test loss: 0.4621, test acc: 0.79, time: 50.75\n",
      "epoch: 2, train loss: 0.4614, train acc: 0.79, test loss: 0.4263, test acc: 0.81, time: 50.72\n",
      "epoch: 3, train loss: 0.4364, train acc: 0.80, test loss: 0.4504, test acc: 0.80, time: 51.22\n",
      "epoch: 4, train loss: 0.4310, train acc: 0.81, test loss: 0.4106, test acc: 0.82, time: 50.83\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "#         net.train()\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature.cuda())\n",
    "        label = Variable(label.cuda())\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "#         scheduler.step()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "#             net.eval()\n",
    "            test_feature = test_feature.cuda()\n",
    "            test_label = test_label.cuda()\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n",
    "#     print('epoch: %d, loss: %.4f, acc: %.2f'\n",
    "#           % (epoch, train_loss.data / n, train_acc / n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
