#!/usr/bin/python
""" 
## Open file
fp = open('label_0.csv', "r")
line = fp.readline()

##

while line:
    line = fp.readline().split(',')[1]
    
    
#    print(type(line))
    print(count)

fp.close()
"""
import tqdm
import os
import time
import re
import pandas as pd
import string
import gensim
import time
import random
import snowballstemmer
import collections
from collections import Counter
from nltk.corpus import stopwords
from itertools import chain

import sys
import csv
 
csv.field_size_limit(sys.maxsize)

train_data = []
test_data = []

count=0
with open('final_all_opcode_sequence_15360_0717.csv', newline='') as csvFile:
	rows = csv.DictReader(csvFile)
	for row in rows:
		#row=row.replace("'", '')
		if count%5==0:
			if row['label']=='1':
				test_data.append([row['op_seq'].replace("'","\""),1])
			else:
				test_data.append([row['op_seq'].replace("'","\""),0])
#			print(type(row['op_seq']))
#			print(type(test_data))
#			print(test_data)
		else:
			if row['label']=='1':
				train_data.append([row['op_seq'].replace("'","\""),1])
			else:
				train_data.append([row['op_seq'].replace("'","\""),0])
		
		count=count+1
		print(count)

def tokenizer(text):
	return [tok.lower() for tok in text.split(' ')]

train_tokenized = []
test_tokenized = []
count=0
for review, score in train_data:
	train_tokenized.append(tokenizer(review))
	count=count+1
	print(count)
count=0
for review, score in test_data:
	test_tokenized.append(tokenizer(review))
	count=count+1
	print(count)

vocab = set(chain(*train_tokenized))
print(vocab)
vocab_size = len(vocab)



