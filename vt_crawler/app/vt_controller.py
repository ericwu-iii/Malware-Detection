from vt_report_downloader import VTReportDownloader
from vt_json_parser import VTJsonParser
from vt_sha_manager import ShaManager
import logging
import sys
import os
import json
import pickle
from datetime import datetime
import pandas as pd
import configparser
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
parpath = os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))
config = configparser.ConfigParser()
config.read(parpath + "/config.ini")
TXT_PATH = config.get("filepath", "txt_path")
PKL_PATH = config.get("filepath", "pkl_path")
DANGERFNAME_PATH = config.get("filepath", "dangername_path")


class VTReportController(object):

    def __init__(self, sha_filename=None):
        self.sha_filename = sha_filename
        if sha_filename:
            self.sha_manager = ShaManager(sha_filename=sha_filename)
        else:
            self.sha_manager = ShaManager()

    def crawl_and_convert_one_report(self, sha256):
        logging.info('crawling vt report: {} ...'.format(sha256))

        _url = 'https://www.virustotal.com/ui/files/{}'.format(sha256)
        _txt_filename = '{}.txt'.format(sha256)

        # crawl report data and save to a txt file
        test = VTReportDownloader(_url)
        test.save_report_2_txt()
        # read txt file
        report = open(TXT_PATH + _txt_filename, 'r').read()
        # convert txt file into json format
        json_respond = json.loads(report)
        # parse the json report into structured data
        tobj = VTJsonParser(json_respond)
        # print(json.dumps(tobj.get_all_report_results(), indent=4))
        json_results = tobj.get_all_report_results()
        # dump results to pickle file
        pickle.dump(json_results, open(PKL_PATH + '{}.pkl'.format(sha256), 'wb'))

    def crawl_all_reports(self, run_times=30):
        while run_times:
            url_list = ['https://www.virustotal.com/ui/files/{}'.format(sha256) for sha256 in self.sha_manager.new_shas]
            print(len(url_list))
            logging.info('Runtimes left: {}'.format(run_times))
            # crawl report data and save to a txt file
            test = VTReportDownloader(_url_list=url_list)
            if_rerun = test.batch_save_report_2_txt()
            self.sha_manager.update_shas()
            if if_rerun:
                run_times -= 1
            else:
                break

    def convert_txt_reports_2_pkls(self):
        for sha256 in self.sha_manager.new_txts:
            logging.info('Converting {} ...'.format(sha256))
            _txt_filename = '{}.txt'.format(sha256)
            report = open(TXT_PATH + _txt_filename, 'r').read()
            # convert txt file into json format
            json_respond = json.loads(report)
            # parse the json report into structured data
            tobj = VTJsonParser(json_respond)
            # print(json.dumps(tobj.get_all_report_results(), indent=4))
            json_results = tobj.get_all_report_results()
            # dump results to pickle file
            # python3:
            pickle.dump(json_results, open(PKL_PATH + '{}.pkl'.format(sha256), 'wb'))
            # python2:
            # pickle.dump(json_results, open(PKL_PATH + '{}.pkl'.format(sha256), 'wb'), protocol=2)


def get_history_info():
    PATH = '/Users/bensonbair/Documents/projects/III/virustotal_crawler/pkl_output_all/'
    sha_manager = ShaManager()
    pkls = sha_manager.get_folder_shas(PATH, 'pkl')
    print(len(pkls))

    sha256_list, creation_date_list, first_submission_date_list, last_analysis_date_list, last_modification_date_list, last_submission_date_list = [], [], [], [], [], []
    kaspersky_list, microsoft_list, trendmicro_list = [], [], []
    c = 0

    for pkl in pkls:
        # print(pkl)
        dic = pickle.load(open('{}{}.pkl'.format(PATH, pkl), 'rb'))
        sha256_list.append(dic['id'])
        creation_date_list.append(dic['history']['creation_date'])
        first_submission_date_list.append(dic['history']['first_submission_date'])
        last_analysis_date_list.append(dic['history']['last_analysis_date'])
        last_modification_date_list.append(dic['history']['last_modification_date'])
        last_submission_date_list.append(dic['history']['last_submission_date'])
        # try:
        kaspersky_list.append(dic['detection_results']['Kaspersky'])
        microsoft_list.append(dic['detection_results']['Microsoft'])
        trendmicro_list.append(dic['detection_results']['TrendMicro'])
        # except:
        #     c += 1

    print(c)

    df = pd.DataFrame({
        'sha256': sha256_list,
        'creation_date': [unittime_2_date(ts) for ts in creation_date_list],
        'first_submission_date': [unittime_2_date(ts) for ts in first_submission_date_list],
        'last_analysis_date': [unittime_2_date(ts) for ts in last_analysis_date_list],
        'last_modification_date': [unittime_2_date(ts) for ts in last_modification_date_list],
        'last_submission_date': [unittime_2_date(ts) for ts in last_submission_date_list],
        'creation_date_ut': creation_date_list,
        # 'first_submission_date_ut': first_submission_date_list,
        # 'last_analysis_date_ut': last_analysis_date_list,
        # 'last_modification_date_ut': last_modification_date_list,
        # 'last_submission_date_ut': last_submission_date_list
        'kaspersky': kaspersky_list,
        'microsoft': microsoft_list,
        'trendmicro': trendmicro_list,
    })
    df = df.sort_values(by='creation_date_ut').reset_index(drop=True)
    df.to_csv(parpath + '/hd5_time_reference_0309.csv', index=False)
    # return df


def unittime_2_date(ut):
    if ut is None:
        return None
    return datetime.utcfromtimestamp(ut).strftime('%Y-%m-%d %H:%M:%S')


def main():
    limit_try = 100
    while limit_try:
        try:
            t = VTReportController(sha_filename='/crawled_hash_set_0330.pkl')
            t.crawl_all_reports(run_times=60)
            t.convert_txt_reports_2_pkls()
            break
        except Exception as e:
            error = (str('--Error on line {}'.format(sys.exc_info()[-1].tb_lineno) + str(e)))
            logging.info("{}".format(error))
            limit_try -= 1
            continue
    t = VTReportController()
    t.convert_txt_reports_2_pkls()


if __name__ == '__main__':
    main()
    # get_history_info()
